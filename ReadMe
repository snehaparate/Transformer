The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.

Jay Alammar Blog to understand Transformer:
https://jalammar.github.io/illustrated-transformer/


Illustrated the "Stanford Sentiment Tree bank task is to predict whether a movie review is positive, negative or neutral."
check out the jupyter file.
